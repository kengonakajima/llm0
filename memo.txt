結果memo


- 1回目

./.venv/bin/python train_transformer.py --steps 1000 --block-size 256 --batch-size 256 --n-layer 4 --n-head 4 --n-embd 256 --lr 6e-4

train: 100% 1000/1000 [12:20<00:00,  1.35it/s, step=1000 loss=1.2675]
[Finished] steps=1000, final_loss=1.2675, elapsed=740.35s, val_loss=1.4777, val_ppl=4.38


結果memo


python3 train_bigram.py --steps 200 --batch-size 8192 --lr 0.03 --temperature 1.0

5000	1.4354


./.venv/bin/python train_transformer.py --steps 1000 --block-size 256 --batch-size 256 --n-layer 4 --n-head 4 --n-embd 256 --lr 6e-4

1000	1.0954



./.venv/bin/python train_transformer.py --steps 2000 --block-size 512 --batch-size 256 --n-layer 6 --n-head 8 --n-embd 512 --lr 3e-4

これは動かない



# 早期停止を入れた。 final_lossが0.6まで下がった
./.venv/bin/python train_transformer.py --steps 10000 --block-size 256 --batch-size 256 --n-layer 4 --n-head 4 --n-embd 256 --lr 6e-4

train:  22% 2199/10000 [23:56<1:24:57,  1.53it/s, step=2000 loss=0.7192]
[Finished] steps=10000, final_loss=0.6303, elapsed=1436.80s, val_loss=2.1621, val_ppl=8.69


# 重み共有（weight tying）をtransformer.pyに実装しました。学習・保存・生成まで動作確認済みです。
  • 変更点
    • TransformerLM.__init__で出力ヘッドと埋め込みの重みを共有
      • self.head.weight = self.token_embed.weight
  • 期待効果
    • パラメータ削減＋学習の安定化
    • 一般にval PPLがわずかに改善することが多い


./.venv/bin/python train_transformer.py --steps 5000 --block-size 256 --batch-size 256 --n-layer 4 --n-head 4 --n-embd 256 --lr 6e-4

train:  44% 2199/5000 [24:19<30:59,  1.51it/s, step=2000 loss=0.6517]  
[Finished] steps=5000, final_loss=0.5669, elapsed=1459.53s, val_loss=2.1766, val_ppl=8.82
[Saved] /Users/ringo/llm0/transformer_latest.pt

確かに下がった。


#   重み共有の次として、少改修で効く3点を入れました。すべて素朴な実装で、学習と生成の質の改善が期待
  できます。
  • 追加した改良
    • ドロップアウト0.1
      • 注意の確率重みと残差、MLP出力に適用
    • ラベルスムージング0.1
      • 交差エントロピーの引数で有効化



./.venv/bin/python train_transformer.py --steps 5000 --block-size 256 --batch-size 256 --n-layer 4 --n-head 4 --n-embd 256 --lr 6e-4
[Training Transformer LM]
Device: mps
Vocab: 65, Block: 256, Layers: 4, Heads: 4, Embd: 256
train:   0% 0/5000 [00:03<?, ?it/s, step=1 loss=4.1987]
[Val] step=1 loss=4.1862 ppl=65.77

--- Sample ---
b$LZZkHUHeyqpj$W!BcjOSQf!LOKeS Je!D;OFAQa;f3UIva.qajEQsOupCWldy:&jXVKNqEe!t--.F O,VTrhX$zrpdqH;RDdeH;O:gK!!uXVVsae$$xtpf:$-cHYwHCG3kH.vg'3xFkk'
qMO:;?gBq3iNqHhns'F3HGGjk?-myYkOavWFGgVll!Jw
BLi-VSxr;rG


train:  48% 2399/5000 [29:42<32:12,  1.35it/s, step=2000 loss=1.6647]  
[Finished] steps=5000, final_loss=1.6260, elapsed=1782.12s, val_loss=1.9594, val_ppl=7.10
[Saved] /Users/ringo/llm0/transformer_latest.pt


lossは増えたけど、val_pplが下がった



#   評価の安定化を実装しました。検証時はランダムではなく、コーパス上を等間隔に区切った固定ウインドウ
  で評価します。
  • 変更
    • eval_loss_and_pplを決定論的な「固定・等間隔ウインドウ評価」に変更
    • 目的: 検証ノイズ低減、早期終了や比較時の安定性向上
  • 使い方
    • そのまま実行でOK。ログの[Val]と終了時のval_loss/val_pplが以前より安定します。



[@mbp2021 llm0]$ ./.venv/bin/python train_transformer.py --steps 5000 --block-size 256 --batch-size 256 --n-layer 4 --n-head 4 --n-embd 256 --lr 6e-4
[Training Transformer LM]
Device: mps
Vocab: 65, Block: 256, Layers: 4, Heads: 4, Embd: 256
train:   0% 0/5000 [00:03<?, ?it/s, step=1 loss=4.1987]
[Val] step=1 loss=4.1870 ppl=65.82

--- Sample ---
ULo-VCdo.!g,dtC kD.p:cWhOOjIxXa;WcgUqI;T-Ibx?UgoMWretJlbmSu!ZEmg'WstQHIgqOvkuYQV??
-axdXrr'ShTClVxV$mwnP..'PeebAqW!tFyi;cdu,w
M!zRvBSshXt3nHvxxeo.zOSGVUb
XlQ-UjJtIXOAG?Ns'R?pdvhi.ROjciokKc&xeY yupvhdZM

train:  48% 2399/5000 [33:01<35:48,  1.21it/s, step=2000 loss=1.6619]  
[Finished] steps=5000, final_loss=1.6254, elapsed=1981.20s, val_loss=1.9319, val_ppl=6.90
[Saved] /Users/ringo/llm0/transformer_latest.pt
